{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0451b7d",
   "metadata": {},
   "source": [
    "In this notebook I plan on creating the necessary matrices that I will need for the recommender system models.\n",
    "\n",
    "I want to create a model which takes the output of both a collabrotive flittering and content based filtering.\n",
    "\n",
    "The collabrotive flittering model will use a dense matrix made from the data in :\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f3040f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (24, 16)\n",
    "\n",
    "from datetime import datetime\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0e77dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>13883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>11690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>11351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>10300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>8983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  artistID  weight\n",
       "0       2        51   13883\n",
       "1       2        52   11690\n",
       "2       2        53   11351\n",
       "3       2        54   10300\n",
       "4       2        55    8983"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ua_w = pd.read_csv(\"data/user_artists.dat\", sep=\"\\t\")\n",
    "ua_w.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "93ea1404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_users(df, split=0.9):\n",
    "    users = df[\"userID\"].unique()\n",
    "\n",
    "    train_users, test_users = train_test_split(users, train_size=split)\n",
    "\n",
    "    return df[df[\"userID\"].isin(train_users)].reset_index(\n",
    "        drop=True), df[~df[\"userID\"].isin(train_users)].reset_index(\n",
    "        drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d0398e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, holdout_fraction=0.1):\n",
    "    \"\"\"Splits a DataFrame into training and test sets.\n",
    "    Args:\n",
    "    df: a dataframe.\n",
    "    holdout_fraction: fraction of dataframe rows to use in the test set.\n",
    "    Returns:\n",
    "    train: dataframe for training\n",
    "    test: dataframe for testing\n",
    "    \"\"\"\n",
    "    test = df.sample(frac=holdout_fraction, replace=False)\n",
    "    train = df[~df.index.isin(test.index)]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "556121df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sparse(df, idx_cols, val_col):\n",
    "    return tf.SparseTensor(df[idx_cols].values,\n",
    "                          df[val_col].values,\n",
    "                          tuple(df[c].max() + 1 for c in idx_cols) \n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6329b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rating_sparse_tensor(ua_w):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ratings_df: a pd.DataFrame with `user_id`, `movie_id` and `rating` columns.\n",
    "    Returns:\n",
    "    a tf.SparseTensor representing the ratings matrix.\n",
    "    \"\"\"\n",
    "    indices = ua_w[['userID', 'artistID']].values\n",
    "    values = ua_w['weight'].values\n",
    "    return tf.SparseTensor(\n",
    "      indices=indices,\n",
    "      values=values,\n",
    "      dense_shape=[ua_w[\"userID\"].nunique(), ua_w[\"artistID\"].nunique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7597d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title CFModel helper class (run this cell)\n",
    "class CFModel(object):\n",
    "    \"\"\"Simple class that represents a collaborative filtering model\"\"\"\n",
    "    def __init__(self, embedding_vars, loss, metrics=None):\n",
    "        \"\"\"Initializes a CFModel.\n",
    "        Args:\n",
    "            embedding_vars: A dictionary of tf.Variables.\n",
    "            loss: A float Tensor. The loss to optimize.\n",
    "            metrics: optional list of dictionaries of Tensors. The metrics in each\n",
    "                dictionary will be plotted in a separate figure during training.\n",
    "        \"\"\"\n",
    "        self._embedding_vars = embedding_vars\n",
    "        self._loss = loss\n",
    "        self._metrics = metrics\n",
    "        self._embeddings = {k: None for k in embedding_vars}\n",
    "        self._session = None\n",
    "\n",
    "    @property\n",
    "    def embeddings(self):\n",
    "        \"\"\"The embeddings dictionary.\"\"\"\n",
    "        return self._embeddings\n",
    "\n",
    "    def train(self, num_iterations=100, learning_rate=1.0, plot_results=True,\n",
    "                        optimizer=tf.train.GradientDescentOptimizer):\n",
    "        \"\"\"Trains the model.\n",
    "        Args:\n",
    "            iterations: number of iterations to run.\n",
    "            learning_rate: optimizer learning rate.\n",
    "            plot_results: whether to plot the results at the end of training.\n",
    "            optimizer: the optimizer to use. Default to GradientDescentOptimizer.\n",
    "        Returns:\n",
    "            The metrics dictionary evaluated at the last iteration.\n",
    "        \"\"\"\n",
    "        with self._loss.graph.as_default():\n",
    "            opt = optimizer(learning_rate)\n",
    "            train_op = opt.minimize(self._loss)\n",
    "            local_init_op = tf.group(\n",
    "                    tf.variables_initializer(opt.variables()),\n",
    "                    tf.local_variables_initializer())\n",
    "            if self._session is None:\n",
    "                self._session = tf.Session()\n",
    "                with self._session.as_default():\n",
    "                    self._session.run(tf.global_variables_initializer())\n",
    "                    self._session.run(tf.tables_initializer())\n",
    "                    tf.train.start_queue_runners()\n",
    "\n",
    "        with self._session.as_default():\n",
    "            local_init_op.run()\n",
    "            iterations = []\n",
    "            metrics = self._metrics or ({},)\n",
    "            metrics_vals = [collections.defaultdict(list) for _ in self._metrics]\n",
    "\n",
    "            # Train and append results.\n",
    "            for i in range(num_iterations + 1):\n",
    "                _, results = self._session.run((train_op, metrics))\n",
    "                if (i % 10 == 0) or i == num_iterations:\n",
    "                    print(\"\\r iteration %d: \" % i + \", \".join(\n",
    "                                [\"%s=%f\" % (k, v) for r in results for k, v in r.items()]),\n",
    "                                end='')\n",
    "                    iterations.append(i)\n",
    "                    for metric_val, result in zip(metrics_vals, results):\n",
    "                        for k, v in result.items():\n",
    "                            metric_val[k].append(v)\n",
    "\n",
    "            for k, v in self._embedding_vars.items():\n",
    "                self._embeddings[k] = v.eval()\n",
    "\n",
    "            if plot_results:\n",
    "                # Plot the metrics.\n",
    "                num_subplots = len(metrics)+1\n",
    "                fig = plt.figure()\n",
    "                fig.set_size_inches(num_subplots*10, 8)\n",
    "                for i, metric_vals in enumerate(metrics_vals):\n",
    "                    ax = fig.add_subplot(1, num_subplots, i+1)\n",
    "                    for k, v in metric_vals.items():\n",
    "                        ax.plot(iterations, v, label=k)\n",
    "                    ax.set_xlim([1, num_iterations])\n",
    "                    ax.legend()\n",
    "            return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e24f5882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution\n",
    "def build_model(ratings, embedding_dim=3, init_stddev=1.):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        ratings: a DataFrame of the ratings\n",
    "        embedding_dim: the dimension of the embedding vectors.\n",
    "        init_stddev: float, the standard deviation of the random initial embeddings.\n",
    "    Returns:\n",
    "        model: a CFModel.\n",
    "    \"\"\"\n",
    "    # Split the ratings DataFrame into train and test.\n",
    "    train_ratings, test_ratings = split_dataframe(ratings)\n",
    "    # SparseTensor representation of the train and test datasets.\n",
    "    A_train = build_rating_sparse_tensor(train_ratings)\n",
    "    A_test = build_rating_sparse_tensor(test_ratings)\n",
    "    # Initialize the embeddings using a normal distribution.\n",
    "    U = tf.Variable(tf.random_normal(\n",
    "            [A_train.dense_shape[0], embedding_dim], stddev=init_stddev))\n",
    "    V = tf.Variable(tf.random_normal(\n",
    "            [A_train.dense_shape[1], embedding_dim], stddev=init_stddev))\n",
    "    print(U, V)\n",
    "    train_loss = sparse_mean_square_error(A_train, U, V)\n",
    "    test_loss = sparse_mean_square_error(A_test, U, V)\n",
    "    metrics = {\n",
    "            'train_error': train_loss,\n",
    "            'test_error': test_loss\n",
    "    }\n",
    "    embeddings = {\n",
    "            \"user_id\": U,\n",
    "            \"movie_id\": V\n",
    "    }\n",
    "    return CFModel(embeddings, train_loss, [metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ab6e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mean_square_error(sparse_ratings, user_embeddings, movie_embeddings):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    sparse_ratings: A SparseTensor rating matrix, of dense_shape [N, M]\n",
    "    user_embeddings: A dense Tensor U of shape [N, k] where k is the embedding\n",
    "      dimension, such that U_i is the embedding of user i.\n",
    "    movie_embeddings: A dense Tensor V of shape [M, k] where k is the embedding\n",
    "      dimension, such that V_j is the embedding of movie j.\n",
    "  Returns:\n",
    "    A scalar Tensor representing the MSE between the true ratings and the\n",
    "      model's predictions.\n",
    "  \"\"\"\n",
    "  predictions = tf.reduce_sum(\n",
    "      tf.gather(user_embeddings, sparse_ratings.indices[:, 0]) *\n",
    "      tf.gather(movie_embeddings, sparse_ratings.indices[:, 1]),\n",
    "      axis=1)\n",
    "  loss = tf.losses.mean_squared_error(sparse_ratings.values, predictions)\n",
    "  return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomSys",
   "language": "python",
   "name": "recomsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
